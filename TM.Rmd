
```{r}
library(tm)
library(topicmodels)
library(reshape2)
library(ggplot2)
library(wordcloud)
library(pals)
library(qdap)
library(circlize)
library(dendextend) 
library(tidytext)
library(SnowballC) 
library(stringi)
library(topicmodels) # for LDA topic modelling 
library(dplyr)
```


#Topic Modelling
```{r}
# function to get & plot the most informative terms by a specificed number
# of topics, using LDA
top_terms_by_topic_LDA <- function(text, # should be a columm from a dataframe
                                   plot = T, 
                                   number_of_topics = 4) # number of topics (4 by default)
{    
    # create a corpus (type of object expected by tm) and document term matrix
    Corpus <- Corpus(VectorSource(text)) # make a corpus object
    DTM <- DocumentTermMatrix(Corpus) # get the count of words/document

    # remove any empty rows in our document term matrix (if there are any 
    # we'll get an error when we try to run our LDA)
    unique_indexes <- unique(DTM$i) # get the index of each unique value
    DTM <- DTM[unique_indexes,] # get a subset of only those indexes
    
    # preform LDA & get the words/topic in a tidy text format
    lda <- LDA(DTM, k = number_of_topics, control = list(seed = 1234))
    topics <- tidy(lda, matrix = "beta") #beta is the parameter of the Dirichlet prior on the per-topic word distribution

    # get the top ten terms for each topic
    top_terms <- topics  %>% # take the topics data frame and..
      group_by(topic) %>% # treat each topic as a different group
      top_n(15, beta) %>% # get the top 10 most informative words
      ungroup() %>% # ungroup
      arrange(topic, -beta) # arrange words in descending informativeness

    # if the user asks for a plot (TRUE by default)
    if(plot == T){
        # plot the top ten terms for each topic in order
        top_terms %>% # take the top terms
          mutate(term = reorder(term, beta)) %>% # sort terms by beta value 
          ggplot(aes(term, beta, fill = factor(topic))) + # plot beta by theme
          geom_col(show.legend = FALSE) + # as a bar plot
          facet_wrap(~ topic, scales = "free") + # which each topic in a seperate plot
          labs(x = NULL, y = "Beta") + # no x label, change y label 
          coord_flip() # turn bars sideways
    }else{ 
        # if the user does not request a plot
        # return a list of sorted terms instead
        return(top_terms)
    }
}
```



#AJ
```{r}
#original text cleaning
AJ <- read_excel("C:/Users/LENOVO/Desktop/AJ.xlsx")
ynhhreviews <- AJ[-grep("\\b\\d+\\b", AJ$text),]
#this code function is to remove non text charachters. It is important if you are working on twitter texts.
ynhhreviews$text <- sapply(ynhhreviews$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
# create a document term matrix to clean
reviewsCorpus <- Corpus(VectorSource(ynhhreviews$text))
#remove punctuation, numbers, lower upper case letters then stem words
ynhh_corpus <- tm_map(reviewsCorpus, content_transformer(removePunctuation))

ynhh_corpus2 <- tm_map(ynhh_corpus, content_transformer(removeNumbers))
ynhh_corpus3 <- tm_map(ynhh_corpus2, content_transformer(stri_trans_tolower))
ynhh_corpus4  <-  tm_map(ynhh_corpus3,stemDocument)

reviewsDTM <- DocumentTermMatrix(ynhh_corpus4)

# convert the document term matrix to a tidytext corpus
reviewsDTM_tidy <- tidy(reviewsDTM)

# I'm going to add my own custom stop words that I don't think will be
# very informative in hospital reviews
custom_stop_words <- tibble(word = c("hospit", "room", "week", "patient", "life", "day", "bed", "someon", "hour", "befor", "time", "floor", "peopl", "becaus", "tri", "star", "alway", "els", "realli", "veri", "pillow","checkpoint","report","jazeera"))
# remove stopwords
reviewsDTM_tidy_cleaned <- reviewsDTM_tidy %>% # take our tidy dtm and...
    anti_join(stop_words, by = c("term" = "word")) %>% # remove English stopwords and...
    anti_join(custom_stop_words, by = c("term" = "word")) # remove my custom stopwords

# reconstruct cleaned documents (so that each word shows up the correct number of times)
cleaned_documents1 <- reviewsDTM_tidy_cleaned %>%
    group_by(document) %>% 
    mutate(terms = toString(rep(term, count))) %>%
    select(document, terms) %>%
    unique()

```

```{r}
top_terms_by_topic_LDA(cleaned_documents1$terms, number_of_topics = 2)
```


#TRT
```{r}
TR <- read_excel("C:/Users/LENOVO/Desktop/TR.xlsx")
#original text cleaning
ynhhreviews <- TR[-grep("\\b\\d+\\b", TR$text),]
#this code function is to remove non text charachters. It is important if you are working on twitter texts.
ynhhreviews$text <- sapply(ynhhreviews$text,function(row) iconv(row, "latin1", "ASCII", sub=""))
# create a document term matrix to clean
reviewsCorpus <- Corpus(VectorSource(ynhhreviews$text))
#remove punctuation, numbers, lower upper case letters then stem words
ynhh_corpus <- tm_map(reviewsCorpus, content_transformer(removePunctuation))

ynhh_corpus2 <- tm_map(ynhh_corpus, content_transformer(removeNumbers))
ynhh_corpus3 <- tm_map(ynhh_corpus2, content_transformer(stri_trans_tolower))
ynhh_corpus4  <-  tm_map(ynhh_corpus3,stemDocument)

reviewsDTM <- DocumentTermMatrix(ynhh_corpus4)

# convert the document term matrix to a tidytext corpus
reviewsDTM_tidy <- tidy(reviewsDTM)

# I'm going to add my own custom stop words that I don't think will be
# very informative in hospital reviews
custom_stop_words <- tibble(word = c("hospit", "room", "week", "patient", "life", "day", "bed", "someon", "hour", "befor", "time", "floor", "peopl", "becaus", "tri", "star", "alway", "els", "realli", "veri", "pillow"))
# remove stopwords
reviewsDTM_tidy_cleaned <- reviewsDTM_tidy %>% # take our tidy dtm and...
    anti_join(stop_words, by = c("term" = "word")) %>% # remove English stopwords and...
    anti_join(custom_stop_words, by = c("term" = "word")) # remove my custom stopwords

# reconstruct cleaned documents (so that each word shows up the correct number of times)
cleaned_documents2 <- reviewsDTM_tidy_cleaned %>%
    group_by(document) %>% 
    mutate(terms = toString(rep(term, count))) %>%
    select(document, terms) %>%
    unique()

```

```{r}
top_terms_by_topic_LDA(cleaned_documents2$terms, number_of_topics = 2)
```


